<!doctype html>
<html class="no-js">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />
<link rel="index" title="Index" href="../../genindex.html" /><link rel="search" title="Search" href="../../search.html" />

    <meta name="generator" content="sphinx-4.4.0, furo 2021.09.08"/>
        <title>Machine Learning and Neural Networks - Introduction to Computer-based Physical Modeling 22 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo.css?digest=c7c65a82b42f6b978e58466c1e9ef2509836d916" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/furo-extensions.css?digest=16fb25fabf47304eee183a5e9af80b1ba98259b1" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  
  }
  body[data-theme="dark"] {
    --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
  }
  @media (prefers-color-scheme: dark) {
    body:not([data-theme="light"]) {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  
    }
  }
</style></head>
  <body>
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" />
      <line x1="4" y1="6" x2="20" y2="6" />
      <line x1="10" y1="12" x2="20" y2="12" />
      <line x1="6" y1="18" x2="20" y2="18" />
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../../index.html"><div class="brand">Introduction to Computer-based Physical Modeling 22 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand centered" href="../../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo" src="../../_static/mona_logo.png" alt="Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Introduction to Computer-based Physical Modeling 22 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="../../search.html" role="search">
  <input class="sidebar-search" placeholder=Search name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Course Information:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/website.html">This Website</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/schedule.html">Course Schedule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/assignments.html">Assignments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/exam.html">Exams</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/resources.html">Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../course-info/instructor.html">Instructor</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/Intro/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/1_Introduction2Jupyter.html">Introduction to Jupyter</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/2_NotebookEditor.html">Notebook editor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/3_EditCells.html">Entering code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro/3_EditCells.html#Entering-Markdown">Entering Markdown</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 1:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L1/overview_1.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/1_variables.html">Variables and types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/2_operators.html">Operators and comparisons</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/3_datatypes.html">Data Types in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L1/4_modules.html">Modules and namespaces</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L1/assignment_1.html">Exercise 1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 2:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L2/overview_2.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/1_numpy.html">NumPy arrays</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/2_plotting.html">Plotting data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L2/3_randomnumbers.html">Random numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L2/assignment_2.html">Exercise 2</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Lecture 3:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L3/overview_3.html">Lecture Contents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/1_input_output.html">Input and output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/2_flowcontrol.html">Flow Control</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/3_functions.html">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../L3/4_exceptions.html">Exceptions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../lectures/L3/assignment_3.html">Exercise 3</a></li>
</ul>

</div>
</div>
      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<div class="admonition note">
<p>This page was generated from <cite>notebooks/L12/2_reinforcement_learning.ipynb</cite>.
<span class="raw-html"><br/><a href="https://mybinder.org/v2/gh/fcichos/CompSoft22/main?urlpath=tree/source/notebooks/L12/2_reinforcement_learning.ipynb"><img alt="Binder badge" src="https://img.shields.io/badge/launch-%20myBinder-red.svg" style="vertical-align:text-bottom"/></a></span> <span class="raw-html"><br/><a href="https://colab.research.google.com/github/fcichos/CompSoft22/blob/main/source/notebooks/L12/2_reinforcement_learning.ipynb"><img alt="Binder badge" src="https://img.shields.io/badge/launch-%20colab-green.svg" style="vertical-align:text-bottom"/></a></span></p>
</div>
<section id="Machine-Learning-and-Neural-Networks">
<h1>Machine Learning and Neural Networks<a class="headerlink" href="#Machine-Learning-and-Neural-Networks" title="Permalink to this headline">¶</a></h1>
<p>We are close to the end of the course and covered different applications of Python to physical problems. The course is not intended to teach the physics, but exercise the application of Python. One field, which is increasingly important also in physics is the field of machine learning. Machine learning is the summarizing term for a number of computational procedures to extract useful information from data. We would like to spend the rest of the course to introduce you into a tiny part of machine
learning. We will do that in a way that you calculate as much as possible in pure Python without any additional packages.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[767]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.constants</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">diags</span>
<span class="kn">from</span> <span class="nn">scipy.fftpack</span> <span class="kn">import</span> <span class="n">fft</span><span class="p">,</span><span class="n">ifft</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span> <span class="k">as</span> <span class="n">sparse</span>
<span class="kn">from</span> <span class="nn">scipy.sparse</span> <span class="kn">import</span> <span class="n">linalg</span> <span class="k">as</span> <span class="n">ln</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span><span class="p">,</span><span class="n">time</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>
<span class="kn">from</span> <span class="nn">ipycanvas</span> <span class="kn">import</span> <span class="n">MultiCanvas</span><span class="p">,</span> <span class="n">hold_canvas</span><span class="p">,</span><span class="n">Canvas</span>


<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format = 'retina'

<span class="c1"># default values for plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s1">'font.size'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'axes.titlesize'</span><span class="p">:</span> <span class="mi">18</span><span class="p">,</span>
                     <span class="s1">'axes.labelsize'</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'axes.labelpad'</span><span class="p">:</span> <span class="mi">14</span><span class="p">,</span>
                     <span class="s1">'lines.linewidth'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                     <span class="s1">'lines.markersize'</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
                     <span class="s1">'xtick.labelsize'</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'ytick.labelsize'</span> <span class="p">:</span> <span class="mi">16</span><span class="p">,</span>
                     <span class="s1">'xtick.top'</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">'xtick.direction'</span> <span class="p">:</span> <span class="s1">'in'</span><span class="p">,</span>
                     <span class="s1">'ytick.right'</span> <span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                     <span class="s1">'ytick.direction'</span> <span class="p">:</span> <span class="s1">'in'</span><span class="p">,})</span>
</pre></div>
</div>
</div>
<section id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Permalink to this headline">¶</a></h2>
<p>Machine learning has its origins long time ago and many of the currently very popular approaches have been developed in the past century. Two things have been stimmulating the current hype of machine learning techniques. One is the computational power that is available already at the level of your smartphone. The second one is the availability of data. Machine learning is divided into different areas, which are denotes as</p>
<ul class="simple">
<li><p>supervised learning: telling the system what is right or wrong</p></li>
<li><p>semi-supervised learning: having only sparse information on what is right or wrong</p></li>
<li><p>unsupervised learning: let the system figure out what is right or wrong</p></li>
</ul>
<p>The graphics below gives a small summary. In our course, we cannot cover all methods. We will focus on <strong>Reinforcement Learning</strong> and <strong>Neural Networks</strong> just to show you, how things could look in Python.</p>
<img alt="../../_images/ml_overview.png" src="../../_images/ml_overview.png">
<p>Image taken from F. Cichos et al. Nature Machine Intelligence (2020).</p>
</img></section>
<section id="Reinforcement-Learning">
<h2>Reinforcement Learning<a class="headerlink" href="#Reinforcement-Learning" title="Permalink to this headline">¶</a></h2>
<p>Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner or agent is not told which actions to take, as in most forms of machine learning, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two
characteristics—trial-and-error search and delayed reward—are the two most important distinguishing features of reinforcement learning.</p>
<p>It has been around since the 1950s but gained momentum only in 2013 with the demonstrations of DeepMind on how to learn play Atari games like pong. The graphic below shows some of its applications in the field of robotics and gaming.</p>
<img alt="overview_rl" src="../../_images/overview_RL.png">
<section id="Markov-Decision-Process">
<h3>Markov Decision Process<a class="headerlink" href="#Markov-Decision-Process" title="Permalink to this headline">¶</a></h3>
<p>The key element of reinforcement learning is the so-called Markov Decision Process. The Markov decision process (MDP) denotes a formalism of planning actions in the face of uncertainty. A MDP consist formally of</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S\)</span>: a set of accessible states in the world</p></li>
<li><p><span class="math notranslate nohighlight">\(D\)</span>: an initial distribution to be in a state</p></li>
<li><p><span class="math notranslate nohighlight">\(P_{sa}\)</span>: transition probability between states</p></li>
<li><p><span class="math notranslate nohighlight">\(A\)</span>: A set of possible actions to take in each state</p></li>
<li><p><span class="math notranslate nohighlight">\(\gamma\)</span>: the discount factor, which is a number between 0 and 1</p></li>
<li><p><span class="math notranslate nohighlight">\(R\)</span>: A reward function</p></li>
</ul>
<p>We begin in an initial state <span class="math notranslate nohighlight">\(s_{i,j}\)</span> drawn from the distribution <span class="math notranslate nohighlight">\(D\)</span>. At each time step <span class="math notranslate nohighlight">\(t\)</span>, we then have to pick an action, for example <span class="math notranslate nohighlight">\(a_1(t)\)</span> , as a result of which our state transitions to some state <span class="math notranslate nohighlight">\(s_{i,j+1}\)</span>. The states do not nessecarily correspond to spatial positions, however, as we talk about the gridworld later we may use this example to understand the procedures.</p>
<img alt="gw_with_path" src="../../_images/gw_with_path.png"/>
<p>By repeatedly picking actions, we traverse some sequence of states</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[s_{0,0}\rightarrow s_{0,1}\rightarrow s_{1,1}+\ldots\]</div></div>
<p>Our total reward is then the sum of discounted rewards along this sequence of states</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[R(s_{0,0})+\gamma R(s_{0,1})+ \gamma^2 R(s_{1,1})+ \ldots\]</div></div>
<p>Here, the discount factor <span class="math notranslate nohighlight">\(\gamma\)</span>, which is typically strictly less than one, causes rewards obtained immediately to be more valuable than those obtained in the future.</p>
<p>In reinforcement learning, our goal is to find a way of choosing actions <span class="math notranslate nohighlight">\(a_0\)</span>,<span class="math notranslate nohighlight">\(a_1, \ldots\)</span> over time, so as to maximize the expected value of the rewards. The sequence of actions that realizes the maximum reward is called the optimal policy <span class="math notranslate nohighlight">\(\pi^{*}\)</span>. A sequence of actions in general is called a policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<section id="Methods-or-RL">
<h4>Methods or RL<a class="headerlink" href="#Methods-or-RL" title="Permalink to this headline">¶</a></h4>
<p>There are different methods available to find the optimal policy. If we know the transition probabilities <span class="math notranslate nohighlight">\(P_{sa}\)</span> the methods are called model-based algorithms. The so-called value interation procedure would be one of those methods, which we, however, do not consider.</p>
<p>If we don’t know the transition probabilities, then its model-free RL. We will have a look at one of those mode-free algorithms, which is Q‐learning.</p>
<p>In Q-learning, the value of an action in a state is measured by its Q-value. The expectation value <span class="math notranslate nohighlight">\(E\)</span> of the rewards with and initial state and action for a given policy is the Q-function or Q-value.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[Q^{\pi}(s,a)=E[R(s_{0},a_{0})+\gamma R(s_{1},a_{1})+ \gamma^2 R(s_{2},a_{2})+ \ldots | s_{0}=s,a_{0}=a,a_{t}=\pi(s_{t})]\]</div></div>
<p>This sounds complicated but is in principle easy. There is a Q-value for all actions of each state. Thus if we have 4 actions an 25 states, we have to store in total 100 Q-values.</p>
<p>For the optimal sequence of actions - for the best way to go - this Q value becomes a maximum.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[Q^{*}(s,a)=\max_{\pi}Q^{\pi}(s,a)\]</div></div>
<p>The policy which gives the sequence of actions to be carried out to get the maximum reward is then calculated by</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\pi^{*}(s)={\rm argmax_{a}}Q^{*}(s,a)\]</div></div>
<p>The <strong>Q-learning</strong> algorithm is now an iterative procedure of updating the Q-value of each state and action which converges to the optimal policy <span class="math notranslate nohighlight">\(\pi^{*}\)</span>. It is given by</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[Q_{t+\Delta t}(s,a)  = Q_t(s,a) + \alpha\big[R(s) + \gamma \max_{a'}Q_t(s',a')-Q_t(s,a)\big]\]</div></div>
<p>This states, that the current Q-value of the current state <span class="math notranslate nohighlight">\(s\)</span> and the taken action <span class="math notranslate nohighlight">\(a\)</span> for the next step is calculated from its current value <span class="math notranslate nohighlight">\(Q_t(s,a)\)</span> plus an update value. This update value is calculated by multiplying the so-called learing rate <span class="math notranslate nohighlight">\(\alpha\)</span> with the reward <span class="math notranslate nohighlight">\(R\)</span> obtained when taking the action plus a discounted value (discounted by <span class="math notranslate nohighlight">\(\gamma\)</span>) when taking the best action in the next state <span class="math notranslate nohighlight">\(\gamma \max_{a'}Q_t(s',a')\)</span>. This is the procedure
we would like to explore in a small Python program, which is not too difficult.</p>
</section>
</section>
</img></section>
<section id="Navigating-a-Grid-World">
<h2>Navigating a Grid World<a class="headerlink" href="#Navigating-a-Grid-World" title="Permalink to this headline">¶</a></h2>
<p>For our Python course we will have a look at the standard problem of reinforcement learning, which is the navigation in a grid world. Each of the grid cells below represents a state <span class="math notranslate nohighlight">\(s\)</span> in which an object could reside. In each of these states, the object can take several actions. If it may step to left, right, up or down, there are 4 actions, which we may call <span class="math notranslate nohighlight">\(a_{1},a_{2},a_{3}\)</span> and <span class="math notranslate nohighlight">\(a_{4}\)</span>.</p>
<p>This image below shows our gridworld, with 25 states, where the shaded state is the goal state where we want the agent to go to independent of its intial state.</p>
<img alt="gridworld" src="../../_images/gridworld.png"/>
<p>In each of these state, we have 4 possible action as depicted below</p>
<img alt="actions" src="../../_images/state_n_action.png"/>
<section id="Initialize-Reinforcement-Learning">
<h3>Initialize Reinforcement Learning<a class="headerlink" href="#Initialize-Reinforcement-Learning" title="Permalink to this headline">¶</a></h3>
<p>At first we would like to initialize our problem. We have as depicted above 25 states, where one state is the goal state. We would like to use 4 actions to move between the states so our Q-value matrix has 100 entries. We would like to give a penalty of <span class="math notranslate nohighlight">\(R=-1\)</span> for all states except for the goal state where we give a reward of <span class="math notranslate nohighlight">\(R=10\)</span>.</p>
<p>Our agent shall learn with a learning rate of <span class="math notranslate nohighlight">\(\alpha=0.5\)</span> and we will discount future rewards with <span class="math notranslate nohighlight">\(\gamma=0.5\)</span>.</p>
<p>There is one tiny detail, which is useful to understand. If we run into a certain strategy and this is not the optimal strategy, it is difficult for the algorithm to choose a different action. Therefore the so called <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy factor is introduced. It tells you at which fraction of events in a state a random action is to be chosen over the action with the larges Q-value. We will set this <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy value to 0.2, meaning that 20% of the actions are chosen randomly.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[805]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_actions</span><span class="o">=</span><span class="mi">4</span>
<span class="n">n_rows</span><span class="o">=</span><span class="n">n_columns</span><span class="o">=</span><span class="mi">5</span>

<span class="n">Q</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">,</span><span class="n">n_actions</span><span class="p">)</span>
<span class="n">R</span><span class="o">=-</span><span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
<span class="n">R</span><span class="p">[</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n_columns</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="mi">10</span>

<span class="n">e_greedy</span><span class="o">=</span><span class="mf">0.2</span>
<span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span>
<span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span>
</pre></div>
</div>
</div>
</section>
<section id="List-of-actions">
<h3>List of actions<a class="headerlink" href="#List-of-actions" title="Permalink to this headline">¶</a></h3>
<p>The actions, which we can take in each state are defined by 2-d vectors here which increase either the row or the column index in our gridworld.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[806]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acl</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</section>
<section id="Initial-state">
<h3>Initial state<a class="headerlink" href="#Initial-state" title="Permalink to this headline">¶</a></h3>
<p>We chose the initial state from which we start randomly. We also initialize a list, where we register the sum of all Q-values. This is helpful to monitor the convergence of our algorithm.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[807]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">curr_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
<span class="n">curr_state</span>
<span class="n">ep</span><span class="o">=</span><span class="mi">0</span>
<span class="n">qsum</span><span class="o">=</span><span class="p">[]</span>
</pre></div>
</div>
</div>
</section>
<section id="Reinforcement-Learning-Loop">
<h3>Reinforcement Learning Loop<a class="headerlink" href="#Reinforcement-Learning-Loop" title="Permalink to this headline">¶</a></h3>
<p>The cell below is all you need for the learning how to navigate the grid world.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[808]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span><span class="o">&gt;</span><span class="n">e_greedy</span><span class="p">:</span>
        <span class="n">action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],:])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>

    <span class="n">next_state</span><span class="o">=</span><span class="n">curr_state</span><span class="o">+</span><span class="n">acl</span><span class="p">[</span><span class="n">action</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">&lt;=</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">&gt;=</span><span class="mi">0</span><span class="p">):</span> <span class="c1">## normal states</span>
        <span class="n">next_action</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],:])</span>
        <span class="n">next_Q</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">next_action</span><span class="p">]</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="o">+</span><span class="n">gamma</span><span class="o">*</span><span class="n">next_Q</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">next_state</span><span class="o">==</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1">## the goal state, episode ends</span>
            <span class="n">next_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
            <span class="n">ep</span><span class="o">+=</span><span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">=</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]</span><span class="o">+</span><span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">curr_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">action</span><span class="p">]);</span>
        <span class="n">next_state</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">([</span><span class="n">n_rows</span><span class="p">,</span><span class="n">n_columns</span><span class="p">])</span>
        <span class="n">ep</span><span class="o">+=</span><span class="mi">1</span>
    <span class="n">qsum</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Q</span><span class="p">))</span>

    <span class="c1">#curr_action=next_action</span>
    <span class="n">curr_state</span><span class="o">=</span><span class="n">next_state</span>
<br/></pre></div>
</div>
</div>
</section>
<section id="Convergence-of-the-Q-learning">
<h3>Convergence of the Q-learning<a class="headerlink" href="#Convergence-of-the-Q-learning" title="Permalink to this headline">¶</a></h3>
<p>The convergence of our learning is best judged from the sum of all Q-values in the matrix. This should converge to a negative value as most of the time our agent is getting the penalty <span class="math notranslate nohighlight">\(R=-1\)</span> and only sparsely <span class="math notranslate nohighlight">\(R=10\)</span> at the goal.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[809]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">qsum</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'transition'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">'$\sum Q$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_L12_2_reinforcement_learning_49_0.png" class="no-scaled-link" src="../../_images/notebooks_L12_2_reinforcement_learning_49_0.png" style="width: 454px; height: 280px;"/>
</div>
</div>
</section>
<section id="Policy">
<h3>Policy<a class="headerlink" href="#Policy" title="Permalink to this headline">¶</a></h3>
<p>The policy is obtained by taking the best actions with the larges Q-value from our Q-matrix.</p>
<div class="math-wrapper"><div class="math notranslate nohighlight">
\[\pi^{*}(s)={\rm argmax_{a}}Q^{*}(s,a)\]</div></div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[810]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[:,:,:],</span><span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">policy</span><span class="p">[</span><span class="n">n_rows</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">n_columns</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">=-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
</section>
<section id="Plot-the-policy">
<h3>Plot the policy<a class="headerlink" href="#Plot-the-policy" title="Permalink to this headline">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[811]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">f</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">y</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">n_rows</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">l</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">n_columns</span><span class="o">*</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">policy</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="p">]</span><span class="o">!=-</span><span class="mi">1</span><span class="p">:</span>
            <span class="n">vec</span><span class="o">=</span><span class="n">acl</span><span class="p">[</span><span class="n">policy</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">l</span><span class="p">]]</span><span class="o">*</span><span class="mi">2</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">!=</span><span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">:</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">vec</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fc</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s2">"k"</span><span class="p">,</span><span class="n">head_width</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">head_length</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span> <span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">rect</span><span class="o">=</span><span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="n">y</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'green'</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">rect</span><span class="p">)</span>
        <span class="n">i</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_rows</span><span class="o">*</span><span class="n">n_columns</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">n_columns</span><span class="p">)</span><span class="n">b</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">6</span><span class="o">*</span><span class="n">n_rows</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">'k'</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">'--'</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">f</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">set_ticklabels</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_L12_2_reinforcement_learning_54_0.png" class="no-scaled-link" src="../../_images/notebooks_L12_2_reinforcement_learning_54_0.png" style="width: 349px; height: 340px;"/>
</div>
</div>
</section>
</section>
<section id="Where-to-go-from-here">
<h2>Where to go from here<a class="headerlink" href="#Where-to-go-from-here" title="Permalink to this headline">¶</a></h2>
<p>If you want to know more about Reinforcement Learning, have a look at the <a class="reference external" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">book</a> of Sutton and Barto.</p>
<script type="application/vnd.jupyter.widget-state+json">
{"state": {}, "version_major": 2, "version_minor": 0}
</script></section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>

        <div class="related-information">
              Copyright &#169; 2022, Frank Cichos |
            Last updated on Apr 28, 2022. |
            Built with <a href="https://www.sphinx-doc.org/">Sphinx</a>
              and
              <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
              <a href="https://github.com/pradyunsg/furo">Furo theme</a>. |
            <a class="muted-link" href="../../_sources/notebooks/L12/2_reinforcement_learning.ipynb.txt"
               rel="nofollow">
              Show Source
            </a>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            Contents
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Machine Learning and Neural Networks</a><ul>
<li><a class="reference internal" href="#Overview">Overview</a></li>
<li><a class="reference internal" href="#Reinforcement-Learning">Reinforcement Learning</a><ul>
<li><a class="reference internal" href="#Markov-Decision-Process">Markov Decision Process</a><ul>
<li><a class="reference internal" href="#Methods-or-RL">Methods or RL</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#Navigating-a-Grid-World">Navigating a Grid World</a><ul>
<li><a class="reference internal" href="#Initialize-Reinforcement-Learning">Initialize Reinforcement Learning</a></li>
<li><a class="reference internal" href="#List-of-actions">List of actions</a></li>
<li><a class="reference internal" href="#Initial-state">Initial state</a></li>
<li><a class="reference internal" href="#Reinforcement-Learning-Loop">Reinforcement Learning Loop</a></li>
<li><a class="reference internal" href="#Convergence-of-the-Q-learning">Convergence of the Q-learning</a></li>
<li><a class="reference internal" href="#Policy">Policy</a></li>
<li><a class="reference internal" href="#Plot-the-policy">Plot the policy</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Where-to-go-from-here">Where to go from here</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/scripts/main.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"equationNumbers": {"autoNumber": "AMS", "useLabelIds": true}}, "tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "tex2jax_ignore|mathjax_ignore|document", "processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>